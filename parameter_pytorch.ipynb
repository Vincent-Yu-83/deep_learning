{"cells":[{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([[0.3365],\n","        [0.2426]], grad_fn=<AddmmBackward0>)\n","OrderedDict([('weight', tensor([[ 0.1824, -0.2105,  0.2959, -0.0728,  0.3397, -0.2215, -0.0778, -0.0849]])), ('bias', tensor([0.0976]))])\n","<class 'torch.nn.parameter.Parameter'>\n","Parameter containing:\n","tensor([0.0976], requires_grad=True)\n","tensor([0.0976])\n","True\n","('weight', torch.Size([8, 4])) ('bias', torch.Size([8]))\n","('0.weight', torch.Size([8, 4])) ('0.bias', torch.Size([8])) ('2.weight', torch.Size([1, 8])) ('2.bias', torch.Size([1]))\n","tensor([0.0976])\n","Sequential(\n","  (0): Sequential(\n","    (block 0): Sequential(\n","      (0): Linear(in_features=4, out_features=8, bias=True)\n","      (1): ReLU()\n","      (2): Linear(in_features=8, out_features=4, bias=True)\n","      (3): ReLU()\n","    )\n","    (block 1): Sequential(\n","      (0): Linear(in_features=4, out_features=8, bias=True)\n","      (1): ReLU()\n","      (2): Linear(in_features=8, out_features=4, bias=True)\n","      (3): ReLU()\n","    )\n","    (block 2): Sequential(\n","      (0): Linear(in_features=4, out_features=8, bias=True)\n","      (1): ReLU()\n","      (2): Linear(in_features=8, out_features=4, bias=True)\n","      (3): ReLU()\n","    )\n","    (block 3): Sequential(\n","      (0): Linear(in_features=4, out_features=8, bias=True)\n","      (1): ReLU()\n","      (2): Linear(in_features=8, out_features=4, bias=True)\n","      (3): ReLU()\n","    )\n","  )\n","  (1): Linear(in_features=4, out_features=1, bias=True)\n",")\n","tensor([-0.4184,  0.2278, -0.3837,  0.0776,  0.1472,  0.1996,  0.1263,  0.3107])\n","tensor([True, True, True, True, True, True, True, True])\n","tensor([True, True, True, True, True, True, True, True])\n","Parameter containing:\n","tensor([[-1.5716, -0.4622, -1.2872],\n","        [-0.0548,  0.4836,  1.1092],\n","        [ 0.4956,  1.4062,  0.3173],\n","        [-0.6017,  1.5214,  0.4398],\n","        [-0.1650, -0.5728, -0.8556]], requires_grad=True)\n","tensor([[0.0000, 0.1582, 0.0000],\n","        [0.0000, 0.9540, 0.0000]])\n"]}],"source":["import torch\n","import torch.nn.functional as F\n","from torch import nn\n","\n","net = nn.Sequential(nn.Linear(4, 8), nn.ReLU(), nn.Linear(8, 1))\n","X = torch.rand(size=(2, 4))\n","print(net(X))\n","\n","# 检查第二个全连接层的参数\n","print(net[2].state_dict())\n","# 提取第二层的bias信息\n","print(type(net[2].bias))\n","print(net[2].bias)\n","print(net[2].bias.data)\n","# 检查第二层weight的梯度\n","print(net[2].weight.grad == None)\n","# 提取特定层参数信息或全部参数信息\n","print(*[(name, param.shape) for name, param in net[0].named_parameters()])\n","print(*[(name, param.shape) for name, param in net.named_parameters()])\n","# 通过层的命名访问参数值\n","print(net.state_dict()['2.bias'].data)\n","\n","# 定义块1\n","def block1():\n","    return nn.Sequential(nn.Linear(4, 8), nn.ReLU(),\n","                         nn.Linear(8, 4), nn.ReLU())\n","# 定义块2\n","def block2():\n","    net = nn.Sequential()\n","    for i in range(4):\n","        # 在这里嵌套\n","        net.add_module(f'block {i}', block1())\n","    return net\n","\n","rgnet = nn.Sequential(block2(), nn.Linear(4, 1))\n","rgnet(X)\n","print(rgnet)\n","# 打印具体位置的参数值\n","print(rgnet[0][1][0].bias.data)\n","\n","# 将所有权重参数初始化为标准差为0.01的高斯随机变量， 且将偏置参数设置为0\n","def init_normal(m):\n","    if type(m) == nn.Linear:\n","        nn.init.normal_(m.weight, mean=0, std=0.01)\n","        # nn.init.constant_(m.weight, 1)\n","        nn.init.zeros_(m.bias)\n","net.apply(init_normal)\n","net[0].weight.data[0], net[0].bias.data[0]\n","\n","# 我们需要给共享层一个名称，以便可以引用它的参数\n","shared = nn.Linear(8, 8)\n","net = nn.Sequential(nn.Linear(4, 8), nn.ReLU(),\n","                    shared, nn.ReLU(),\n","                    shared, nn.ReLU(),\n","                    nn.Linear(8, 1))\n","net(X)\n","# 检查参数是否相同\n","print(net[2].weight.data[0] == net[4].weight.data[0])\n","net[2].weight.data[0, 0] = 100\n","# 确保它们实际上是同一个对象，而不只是有相同的值\n","print(net[2].weight.data[0] == net[4].weight.data[0])\n","\n","# 给网络初始化参数\n","class MyLinear(nn.Module):\n","    def __init__(self, in_units, units):\n","        super().__init__()\n","        self.weight = nn.Parameter(torch.randn(in_units, units))\n","        self.bias = nn.Parameter(torch.randn(units,))\n","    def forward(self, X):\n","        linear = torch.matmul(X, self.weight.data) + self.bias.data\n","        return F.relu(linear)\n","\n","linear = MyLinear(5, 3)\n","print(linear.weight)\n","print(linear(torch.rand(2, 5)))"]}],"metadata":{"kernelspec":{"display_name":"python3.8","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.19"}},"nbformat":4,"nbformat_minor":2}
